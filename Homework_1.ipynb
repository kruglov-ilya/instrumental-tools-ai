{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание №1: линейная регрессия и векторное дифференцирование (10 баллов).\n",
    "\n",
    "* Максимальное количество баллов за задания в ноутбуке - 11, но больше 10 оценка не ставится, поэтому для получения максимальной оценки можно сделать не все задания.\n",
    "\n",
    "* Некоторые задания будут по вариантам (всего 4 варианта). Чтобы выяснить свой вариант, посчитайте количество букв в своей фамилии, возьмете остаток от деления на 4 и прибавьте 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Многомерная линейная регрессия из sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим многомерную регрессию из sklearn для стандартного датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 100) (10000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(n_samples = 10000)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас 10000 объектов и 100 признаков. Для начала решим задачу аналитически \"из коробки\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0150149549608656e-25\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "reg = LinearRegression().fit(X, y)\n",
    "print(mean_squared_error(y, reg.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь попробуем обучить линейную регрессию методом градиентного спуска \"из коробки\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5067096265590522e-25\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "reg = SGDRegressor(alpha=1e-15, tol=1e-25, max_iter=10000).fit(X, y)\n",
    "print(mean_squared_error(y, reg.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задание 1 (0.5 балла).*** Объясните, чем вызвано различие двух полученных значений метрики?\n",
    "\n",
    "***Задание 2 (0.5 балла).*** Подберите гиперпараметры в методе градиентного спуска так, чтобы значение MSE было близко к значению MSE, полученному при обучении LinearRegression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LinearRegression использует поиск коэфициентов аналитическим способом, что даёт максимальную точность, но требует продолжительных вычислений\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ваша многомерная линейная регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задание 3 (5 баллов)***. Напишите собственную многомерную линейную регрессию, оптимизирующую MSE методом *градиентного спуска*. Для этого используйте шаблонный класс. \n",
    "\n",
    "Критерий останова: либо норма разности весов на текущей и предыдущей итерациях меньше определенного значения (первый и третий варианты), либо модуль разности функционалов качества (MSE) на текущей и предыдущей итерациях меньше определенного значения (второй и четвертый варианты). Также предлагается завершать обучение в любом случае, если было произведено слишком много итераций.\n",
    "\n",
    "***Задание 4 (2 балла)***. Добавьте l1 (первый и второй варианты) или l2 (третий и четвертый варианты) регуляризацию. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(object):\n",
    "    def __init__(self, alpha=1e-3, l1_ratio=0, l2_ratio=0, tol=0.001, max_iter=1000, stop_by='weights'):\n",
    "        '''\n",
    "        alpha — шаг обучения\n",
    "        l1_ratio — коэффициент регуляризации L1\n",
    "        l2_ratio — коэффициент регуляризации L2\n",
    "        tol — порог останова\n",
    "        max_iter — максимальное число итераций\n",
    "        stop_by — критерий останова: 'weights' или 'loss'\n",
    "        '''\n",
    "        self.alpha = alpha\n",
    "        self.l1_ratio = l1_ratio\n",
    "        self.l2_ratio = l2_ratio\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "        self.stop_by = stop_by  # 'weights' or 'loss'\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def _mse(self, X, y):\n",
    "        '''\n",
    "        Вычисление среднеквадратичной ошибки\n",
    "        '''\n",
    "        predictions = X @ self.weights + self.bias\n",
    "        return np.mean((predictions - y) ** 2)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0.0\n",
    "\n",
    "        prev_loss = float('inf')\n",
    "        prev_weights = np.copy(self.weights)\n",
    "\n",
    "        for i in range(self.max_iter):\n",
    "            y_pred = X @ self.weights + self.bias\n",
    "            error = y_pred - y\n",
    "\n",
    "            l1 = self.l1_ratio * np.sign(self.weights)\n",
    "            l2 = 2 * self.l2_ratio * self.weights\n",
    "\n",
    "            # Градиенты\n",
    "            dw = (2 / n_samples) * (X.T @ error) + l1 + l2\n",
    "            db = (2 / n_samples) * np.sum(error)\n",
    "\n",
    "            # Обновление весов\n",
    "            self.weights -= self.alpha * dw\n",
    "            self.bias -= self.alpha * db\n",
    "\n",
    "            # Критерии останова\n",
    "            if self.stop_by == 'weights':\n",
    "                weight_diff = np.linalg.norm(self.weights - prev_weights)\n",
    "                if weight_diff < self.tol:\n",
    "                    break\n",
    "                prev_weights = np.copy(self.weights)\n",
    "            elif self.stop_by == 'loss':\n",
    "                current_loss = np.mean(error ** 2)\n",
    "                if abs(prev_loss - current_loss) < self.tol:\n",
    "                    break\n",
    "                prev_loss = current_loss\n",
    "\n",
    "    def predict(self, X):\n",
    "        return X @ self.weights + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00013313831140211477\n",
      "You are amazing! Great work!\n"
     ]
    }
   ],
   "source": [
    "my_reg = LinearRegression(alpha=0.04, tol=1e-3, max_iter=1000)\n",
    "my_reg.fit(X, y)\n",
    "print(mean_squared_error(y, my_reg.predict(X)))\n",
    "assert mean_squared_error(y, my_reg.predict(X)) < 1e-3\n",
    "print('You are amazing! Great work!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задание 5 (1 балл)***. Обучите линейную регрессию из коробки\n",
    "\n",
    "* с l1-регуляризацией (from sklearn.linear_model import Lasso, **первый и второй вариант**) или с l2-регуляризацией (from sklearn.linear_model import Ridge, **третий и четвертый вариант**)\n",
    "* со значением параметра регуляризации **0.1 - для первого и третьего варианта, 0.01 - для второго и четвертого варианта**. \n",
    "\n",
    "Обучите вашу линейную регрессию с тем же значением параметра регуляризации и сравните результаты. Сделайте выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Lasso: 0.01617951779635554\n",
      "MSE наша реализация: 0.004119128564820983\n",
      "\n",
      "Количество ненулевых весов в Lasso: 10\n",
      "Количество ненулевых весов в нашей реализации: 100\n"
     ]
    }
   ],
   "source": [
    "#your code here\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso_reg = Lasso(alpha=0.04, tol=1e-3, max_iter=1000)\n",
    "lasso_reg.fit(X, y)\n",
    "lasso_mse = mean_squared_error(y, lasso_reg.predict(X))\n",
    "\n",
    "my_reg = LinearRegression(alpha=0.04, l1_ratio=0.04, tol=1e-3, max_iter=1000)\n",
    "my_reg.fit(X, y)\n",
    "my_mse = mean_squared_error(y, my_reg.predict(X))\n",
    "\n",
    "print(f\"MSE Lasso: {lasso_mse}\")\n",
    "print(f\"MSE наша реализация: {my_mse}\")\n",
    "\n",
    "# Сравниваем количество ненулевых весов\n",
    "print(f\"\\nКоличество ненулевых весов в Lasso: {np.sum(lasso_reg.coef_ != 0)}\")\n",
    "print(f\"Количество ненулевых весов в нашей реализации: {np.sum(my_reg.weights != 0)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наше решение является решением через градиентный спуск, а Lasso использует координатный спуск с вычислением аналитического решения для каждого веса в отдельности, из-за чего качество обучения из коробки для сгенерированной выборки выше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задание 6* (1 балл).***\n",
    "Пусть $P, Q \\in \\mathbb{R}^{n\\times n}$. Найдите $\\nabla_Q tr(PQ)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задание 7* (1 балл).***\n",
    "Пусть $x, y \\in \\mathbb{R}^{n}, M \\in \\mathbb{R}^{n\\times n}$. Найдите $\\nabla_M x^T M y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решения заданий 6 и 7 можно написать на листочке и отправить в anytask вместе с заполненным ноутбуком."
   ]
  },
{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Решение задания 6\n",
    "\n",
    "Найдем $\\nabla_Q tr(PQ)$:\n",
    "\n",
    "1. След матрицы $tr(PQ)$ можно записать как:\n",
    "$$tr(PQ) = \\sum_{i=1}^n \\sum_{j=1}^n P_{ij}Q_{ji}$$\n",
    "\n",
    "2. Частная производная по элементу $Q_{kl}$:\n",
    "$$\\frac{\\partial tr(PQ)}{\\partial Q_{kl}} = P_{lk}$$\n",
    "\n",
    "3. Следовательно, градиент:\n",
    "$$\\nabla_Q tr(PQ) = P^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Решение задания 7\n",
    "\n",
    "Найдем $\\nabla_M x^T M y$:\n",
    "\n",
    "1. Выражение $x^T M y$ можно записать как:\n",
    "$$x^T M y = \\sum_{i=1}^n \\sum_{j=1}^n x_i M_{ij} y_j$$\n",
    "\n",
    "2. Частная производная по элементу $M_{kl}$:\n",
    "$$\\frac{\\partial (x^T M y)}{\\partial M_{kl}} = x_k y_l$$\n",
    "\n",
    "3. Следовательно, градиент:\n",
    "$$\\nabla_M x^T M y = x y^T$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
